{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network implementation in PyTorch\n",
    "\n",
    "This notebook is inspired by the Andrew Ng's amazing Coursera course on Deep learning. The dataset we will be using the train the model on is the MNIST dataset which one of the default datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset with 60000 train data and 10000 test data\n",
      "Type of data in dataset: <class 'torch.Tensor'> AND <class 'int'>\n",
      "Input tensor image dimensions: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#Device setting \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Import MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=False,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "input_tensor, label = train_dataset[0]\n",
    "print('MNIST dataset with {} train data and {} test data'.format(len(train_dataset), len(test_dataset)))\n",
    "print('Type of data in dataset: {} AND {}'.format(type(input_tensor), type(label)))\n",
    "print('Input tensor image dimensions: {}'.format(input_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 254
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the data\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# look at one random sample\n",
    "random_index = np.random.choice(len(train_dataset))\n",
    "input_tesor, label = train_dataset[random_index]\n",
    "\n",
    "#Visualize only the one channel -- \n",
    "plt.imshow(input_tesor[0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\");\n",
    "print('Digit label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyper-parameters for the fully connected Neural network \n",
    "input_size = 784 # Image input for the digits - 28 x 28 x 1 (W-H-C) -- flattened in the end before being fed in the NN \n",
    "num_hidden_layers = 1\n",
    "hidden_layer_size = 100\n",
    "num_classes = 10 \n",
    "num_epochs = 100 \n",
    "batch_size = 100 \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 253,
       "width": 318
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert dataset to a dataloader class for ease of doing batching and SGD operations \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers = 2)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                        batch_size = batch_size, \n",
    "                        num_workers = 2)\n",
    "\n",
    "#Take a look at one batch \n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "#Plotting first 4 digits in the dataset: \n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(samples[i][0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have defined a batch-size of 100 for the training dataset with the samples as seen here to be of size = `100 x 1 x 28 x 28`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_layers, hidden_layer_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features = input_size, out_features = hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        if (self.num_hidden_layers-1) > 1:\n",
    "            #print('here')\n",
    "            self.L_hidden = nn.ModuleList( [nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size) for _ in range(num_hidden_layers-1)] )\n",
    "            self.relu_hidden = nn.ModuleList( [nn.ReLU() for _ in range(num_hidden_layers-1)] )\n",
    "        else:\n",
    "            self.L2 = nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size)\n",
    "        self.L_out = nn.Linear(in_features = hidden_layer_size, out_features = num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.L1(x))\n",
    "        if (self.num_hidden_layers-1) > 1:\n",
    "            #print('computing for multiple layers')\n",
    "            for L_hidden, relu_hidden in zip(self.L_hidden, self.relu_hidden):\n",
    "                out = relu_hidden(L_hidden(out))\n",
    "        else:\n",
    "            out = self.relu(self.L2(out))\n",
    "        out = self.L_out(out) #No softmax or cross-entropy activation just the output from linear transformation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size=input_size, num_hidden_layers=num_hidden_layers, hidden_layer_size=hidden_layer_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 500, step 300/600, loss = 0.0105\n",
      "epoch 1 / 500, step 600/600, loss = 0.0608\n",
      "epoch 11 / 500, step 300/600, loss = 0.0032\n",
      "epoch 11 / 500, step 600/600, loss = 0.0219\n",
      "epoch 21 / 500, step 300/600, loss = 0.1295\n",
      "epoch 21 / 500, step 600/600, loss = 0.0130\n"
     ]
    }
   ],
   "source": [
    "#Loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss() #This is implement softmax activation for us so it is not implemented in the model \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (image_tensors, labels) in enumerate(train_loader):\n",
    "        #image tensor = 100, 1, 28, 28 --> 100, 784 input needed \n",
    "        image_input_to_NN = image_tensors.view(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass \n",
    "        outputs = model(image_input_to_NN)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Backward \n",
    "        optimizer.zero_grad() #Detach and flush the gradients \n",
    "        loss.backward() #Backward gradients evaluation \n",
    "        optimizer.step() #To update the weights/parameters in the NN \n",
    "        \n",
    "        if (epoch) % 10 == 0 and (i+1) % 300 == 0: \n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test \n",
    "with torch.no_grad():\n",
    "    n_correct = 0 \n",
    "    n_samples = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item() #For each correction prediction we add the correct samples \n",
    "    acc = 100 * n_correct / n_samples\n",
    "    print(f'Accuracy = {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
