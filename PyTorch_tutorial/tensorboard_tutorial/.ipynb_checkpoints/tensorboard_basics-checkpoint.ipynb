{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard tutorial\n",
    "\n",
    "Using MNIST data NN implementation in PyTorch for playing around TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "#%matplotlib inline \n",
    "#%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"~/runs/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset with 60000 train data and 10000 test data\n",
      "Type of data in dataset: <class 'torch.Tensor'> AND <class 'int'>\n",
      "Input tensor image dimensions: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#Device setting \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Import MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=False,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "input_tensor, label = train_dataset[0]\n",
    "print('MNIST dataset with {} train data and {} test data'.format(len(train_dataset), len(test_dataset)))\n",
    "print('Type of data in dataset: {} AND {}'.format(type(input_tensor), type(label)))\n",
    "print('Input tensor image dimensions: {}'.format(input_tensor.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Plot the data\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# look at one random sample\n",
    "random_index = np.random.choice(len(train_dataset))\n",
    "input_tesor, label = train_dataset[random_index]\n",
    "\n",
    "#Visualize only the one channel -- \n",
    "plt.imshow(input_tesor[0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\");\n",
    "print('Digit label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyper-parameters for the fully connected Neural network \n",
    "input_size = 784 # Image input for the digits - 28 x 28 x 1 (W-H-C) -- flattened in the end before being fed in the NN \n",
    "num_hidden_layers = 1\n",
    "hidden_layer_size = 100\n",
    "num_classes = 10 \n",
    "num_epochs = 10\n",
    "batch_size = 100 \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert dataset to a dataloader class for ease of doing batching and SGD operations \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers = 2)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                        batch_size = batch_size, \n",
    "                        num_workers = 2)\n",
    "\n",
    "#Take a look at one batch \n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "#Plotting first 4 digits in the dataset: \n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(samples[i][0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(samples)\n",
    "writer.add_image('mnist_images',img_grid)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have defined a batch-size of 100 for the training dataset with the samples as seen here to be of size = `100 x 1 x 28 x 28`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_layers, hidden_layer_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features = input_size, out_features = hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        if (self.num_hidden_layers-1) > 1:\n",
    "            #print('here')\n",
    "            self.L_hidden = nn.ModuleList( [nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size) for _ in range(num_hidden_layers-1)] )\n",
    "            self.relu_hidden = nn.ModuleList( [nn.ReLU() for _ in range(num_hidden_layers-1)] )\n",
    "        else:\n",
    "            self.L2 = nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size)\n",
    "        self.L_out = nn.Linear(in_features = hidden_layer_size, out_features = num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.L1(x))\n",
    "        if (self.num_hidden_layers-1) > 1:\n",
    "            #print('computing for multiple layers')\n",
    "            for L_hidden, relu_hidden in zip(self.L_hidden, self.relu_hidden):\n",
    "                out = relu_hidden(L_hidden(out))\n",
    "        else:\n",
    "            out = self.relu(self.L2(out))\n",
    "        out = self.L_out(out) #No softmax or cross-entropy activation just the output from linear transformation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size=input_size, num_hidden_layers=num_hidden_layers, hidden_layer_size=hidden_layer_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss() #This is implement softmax activation for us so it is not implemented in the model \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (image_tensors, labels) in enumerate(train_loader):\n",
    "        #image tensor = 100, 1, 28, 28 --> 100, 784 input needed \n",
    "        image_input_to_NN = image_tensors.view(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass \n",
    "        outputs = model(image_input_to_NN)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Backward \n",
    "        optimizer.zero_grad() #Detach and flush the gradients \n",
    "        loss.backward() #Backward gradients evaluation \n",
    "        optimizer.step() #To update the weights/parameters in the NN \n",
    "        \n",
    "        if (epoch) % 10 == 0 and (i+1) % 300 == 0: \n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test \n",
    "with torch.no_grad():\n",
    "    n_correct = 0 \n",
    "    n_samples = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item() #For each correction prediction we add the correct samples \n",
    "    acc = 100 * n_correct / n_samples\n",
    "    print(f'Accuracy = {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
