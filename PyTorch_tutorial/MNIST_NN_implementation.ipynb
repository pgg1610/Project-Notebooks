{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network implementation in PyTorch\n",
    "\n",
    "This notebook is inspired by the Andrew Ng's amazing Coursera course on Deep learning. The dataset we will be using the train the model on is the MNIST dataset which one of the default datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset with 60000 train data and 10000 test data\n",
      "Type of data in dataset: <class 'torch.Tensor'> AND <class 'int'>\n",
      "Input tensor image dimensions: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#Device setting \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Import MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=False,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "input_tensor, label = train_dataset[0]\n",
    "print('MNIST dataset with {} train data and {} test data'.format(len(train_dataset), len(test_dataset)))\n",
    "print('Type of data in dataset: {} AND {}'.format(type(input_tensor), type(label)))\n",
    "print('Input tensor image dimensions: {}'.format(input_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 254
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the data\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# look at one random sample\n",
    "random_index = np.random.choice(len(train_dataset))\n",
    "input_tesor, label = train_dataset[random_index]\n",
    "\n",
    "#Visualize only the one channel -- \n",
    "plt.imshow(input_tesor[0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\");\n",
    "print('Digit label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyper-parameters for the fully connected Neural network \n",
    "input_size = 784 # Image input for the digits - 28 x 28 x 1 (W-H-C) -- flattened in the end before being fed in the NN \n",
    "num_hidden_layers = 1\n",
    "hidden_layer_size = 100\n",
    "num_classes = 10 \n",
    "num_epochs = 10\n",
    "batch_size = 100 \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 253,
       "width": 318
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert dataset to a dataloader class for ease of doing batching and SGD operations \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers = 2)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                        batch_size = batch_size, \n",
    "                        num_workers = 2)\n",
    "\n",
    "#Take a look at one batch \n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "#Plotting first 4 digits in the dataset: \n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(samples[i][0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have defined a batch-size of 100 for the training dataset with the samples as seen here to be of size = `100 x 1 x 28 x 28`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_layers, hidden_layer_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features = input_size, out_features = hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        if (self.num_hidden_layers-1) > 1:\n",
    "            #print('here')\n",
    "            self.L_hidden = nn.ModuleList( [nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size) for _ in range(num_hidden_layers-1)] )\n",
    "            self.relu_hidden = nn.ModuleList( [nn.ReLU() for _ in range(num_hidden_layers-1)] )\n",
    "        else:\n",
    "            self.L2 = nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size)\n",
    "        self.L_out = nn.Linear(in_features = hidden_layer_size, out_features = num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.L1(x))\n",
    "        if (self.num_hidden_layers-1) > 1:\n",
    "            #print('computing for multiple layers')\n",
    "            for L_hidden, relu_hidden in zip(self.L_hidden, self.relu_hidden):\n",
    "                out = relu_hidden(L_hidden(out))\n",
    "        else:\n",
    "            out = self.relu(self.L2(out))\n",
    "        out = self.L_out(out) #No softmax or cross-entropy activation just the output from linear transformation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size=input_size, num_hidden_layers=num_hidden_layers, hidden_layer_size=hidden_layer_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 10, step 300/600, loss = 0.1331\n",
      "epoch 1 / 10, step 600/600, loss = 0.0913\n"
     ]
    }
   ],
   "source": [
    "#Loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss() #This is implement softmax activation for us so it is not implemented in the model \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (image_tensors, labels) in enumerate(train_loader):\n",
    "        #image tensor = 100, 1, 28, 28 --> 100, 784 input needed \n",
    "        image_input_to_NN = image_tensors.view(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass \n",
    "        outputs = model(image_input_to_NN)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Backward \n",
    "        optimizer.zero_grad() #Detach and flush the gradients \n",
    "        loss.backward() #Backward gradients evaluation \n",
    "        optimizer.step() #To update the weights/parameters in the NN \n",
    "        \n",
    "        if (epoch) % 10 == 0 and (i+1) % 300 == 0: \n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.14%\n"
     ]
    }
   ],
   "source": [
    "#Test \n",
    "with torch.no_grad():\n",
    "    n_correct = 0 \n",
    "    n_samples = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item() #For each correction prediction we add the correct samples \n",
    "    acc = 100 * n_correct / n_samples\n",
    "    print(f'Accuracy = {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
