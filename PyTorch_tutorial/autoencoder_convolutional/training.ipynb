{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ConvVAE on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "\n",
    "import src.model as model\n",
    "from src.utils import train, validate, save_reconstructed_image, create_gifs, plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the model\n",
    "model = model.ConvVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the learning parameters\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to save all the reconstructed images in PyTorch grid format\n",
    "grid_images = []\n",
    "\n",
    "#Convert original images (28 x 28) to (32 x 32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset with 60000 train data and 10000 test data\n",
      "Type of data in dataset: <class 'torch.Tensor'> AND <class 'int'>\n",
      "Input tensor image dimensions: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#Import MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='input/',\n",
    "                                           train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.MNIST(root='input/',\n",
    "                                           train=False,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "input_tensor, label = train_dataset[0]\n",
    "print('MNIST dataset with {} train data and {} test data'.format(len(train_dataset), len(val_dataset)))\n",
    "print('Type of data in dataset: {} AND {}'.format(type(input_tensor), type(label)))\n",
    "print('Input tensor image dimensions: {}'.format(input_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the data\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# look at one random sample\n",
    "random_index = np.random.choice(len(train_dataset))\n",
    "input_tesor, label = train_dataset[random_index]\n",
    "\n",
    "#Visualize only the one channel -- \n",
    "plt.imshow(input_tesor[0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\");\n",
    "print('Digit label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert dataset to a dataloader class for ease of doing batching and SGD operations \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers = 2)\n",
    "\n",
    "val_loader = DataLoader(dataset = val_dataset,\n",
    "                        batch_size = batch_size, \n",
    "                        num_workers = 2)\n",
    "\n",
    "#Take a look at one batch \n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "#Plotting first 4 digits in the dataset: \n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap=mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = 1 # MNIST dataset has grayscale digit image \n",
    "init_channels = 8 # Initial number of filters used by the ConvNet\n",
    "kernel_size = 4 # Kernel size for the Conv Filter \n",
    "latent_dimensions = 16 # Dimensionality of the bottleneck layer \n",
    "\n",
    "enc1 = nn.Conv2d(\n",
    "in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, \n",
    "stride=1, padding=1)\n",
    "\n",
    "enc2 = nn.Conv2d(\n",
    "in_channels=init_channels, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "stride=1, padding=1)\n",
    "\n",
    "enc3 = nn.Conv2d(\n",
    "in_channels=init_channels*2, out_channels=init_channels*3, kernel_size=kernel_size, \n",
    "stride=2, padding=1)\n",
    "\n",
    "enc4 = nn.Conv2d(\n",
    "in_channels=init_channels*3, out_channels=64, kernel_size=kernel_size, \n",
    "stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 27, 27])\n",
      "torch.Size([64, 16, 26, 26])\n",
      "torch.Size([64, 24, 13, 13])\n",
      "torch.Size([64, 64, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "x = enc1(samples)\n",
    "print(x.shape)\n",
    "x = enc2(x)\n",
    "print(x.shape)\n",
    "x = enc3(x)\n",
    "print(x.shape)\n",
    "x = enc4(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, _, _, _ = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/937 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "\n",
    "    train_epoch_loss = train(model, train_loader, train_dataset, device, optimizer, criterion)\n",
    "\n",
    "    valid_epoch_loss, reconstructed_images = validate(model, val_loader, val_dataset, device, criterion)\n",
    "\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    \n",
    "    # save the reconstructed images from the validation loop\n",
    "    save_reconstructed_images(recon_images, epoch+1)\n",
    "\n",
    "    # convert the reconstructed images to PyTorch image grid format\n",
    "    image_grid = make_grid(recon_images.detach().cpu())\n",
    "    grid_images.append(image_grid)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {valid_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
