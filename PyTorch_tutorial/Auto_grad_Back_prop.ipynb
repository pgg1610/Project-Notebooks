{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating gradients in PyTorch\n",
    "\n",
    "Autograd in PyTorch does auto-differentiation which make evaluating forward and backward prop easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6867, -0.3665, -1.1250], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad` which is **False** by-default, once true will help in tracking the gradients for the particular variable. This will also allow for PyTorch to generate a _computational graph_ whenever we do operations for that variable. \n",
    "\n",
    "To get rid of the gradient trailing and do some operations we can do it in 3 ways: \n",
    "```python\n",
    "1. x.requires_grad_(False)\n",
    "2. x.detach() \n",
    "3. with torch.no_grad():\n",
    "    do operations here \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.6867, 1.6335, 0.8750], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2 \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` has an attribute called `grad_fn` which stores a gradient function here as the `AddBackward0` which will be used to calculate the gradient when doing the back-propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.4362,  5.3369,  1.5311], grad_fn=<MulBackward0>) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "print(z, z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.4362,  5.3369,  1.5311], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#z = z.mean()\n",
    "v = torch.tensor([0.1,1.0,0.001],dtype=torch.float64)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradient -- in this case it will be gradient of z wrt x: \n",
    "$$\\nabla Z = \\frac{\\partial Z}{\\partial X}$$\n",
    "\n",
    "`z.backward()` is the method to calculate the gradients upto x that is the base variable \n",
    "\n",
    "`x.grad` is the cache where the gradients are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0747e+00, 6.5342e+00, 3.4999e-03])\n"
     ]
    }
   ],
   "source": [
    "z.backward(v) #dz/dx if the final function is not a scalar it should be multiplied with a vector to make one. Since the final grad is Jacobian x vector \n",
    "print(x.grad) #Here the values of the gradients are accumulated graadients with values added up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: \n",
    "`.grad` or the gradient cache for a variable accumulates all the gradient evaluations for each loop. Hence it needs to be cleared after each epoch run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "Model ouput: 9.0\n",
      "dZ/dW :tensor([3., 3., 3.])\n",
      "Model ouput: 9.0\n",
      "dZ/dW :tensor([6., 6., 6.])\n",
      "Model ouput: 9.0\n",
      "dZ/dW :tensor([9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "#Toy example\n",
    "weights = torch.ones(3, requires_grad=True)\n",
    "print(weights)\n",
    "\n",
    "#Dummy training loop\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum() \n",
    "    print('Model ouput: {}'.format(model_output))\n",
    "    model_output.backward() #dZ/dw\n",
    "    print('dZ/dW :{}'.format(weights.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen for each epoch the gradient `dZ/dW` gets accumulated in the `.grad` cache. Which is not what we need. Since the gradient in all the cases should be same i.e. 3. We must empty the cache after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "Model ouput: 9.0\n",
      "dZ/dW :tensor([3., 3., 3.])\n",
      "Model ouput: 9.0\n",
      "dZ/dW :tensor([3., 3., 3.])\n",
      "Model ouput: 9.0\n",
      "dZ/dW :tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#Toy example\n",
    "weights = torch.ones(3, requires_grad=True)\n",
    "print(weights)\n",
    "\n",
    "#Dummy training loop\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum() \n",
    "    print('Model ouput: {}'.format(model_output))\n",
    "    model_output.backward() #dZ/dw\n",
    "    print('dZ/dW :{}'.format(weights.grad))\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea will be used when we work wit the NN modules in the PyTorch and use a built-in optimizer function to do gradient descent on them. \n",
    "```python\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackProp example\n",
    "\n",
    "Implement forward and backward prop for a simple linear regression example. \n",
    "$$ y = wx $$\n",
    "$$ loss = (y^{known} - wx)^{2} $$\n",
    "When we call for backward pass -- following gradients will be computed as per chain rule: \n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor(2.0)\n",
    "x = torch.tensor(1.0)\n",
    "#Create a weight array -- this requires grad \n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#Forward pass to compute the loss value \n",
    "loss = (y - torch.mul(x,w)) ** 2 \n",
    "print(loss)\n",
    "\n",
    "#Backward pass \n",
    "loss.backward() #Whole gradient with chain rule computed here \n",
    "print(w.grad)\n",
    "\n",
    "#If gradient descent -- We will update the weights here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
